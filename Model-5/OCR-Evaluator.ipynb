{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating OCR Models\n",
    "### TODO - Finish abstract class of cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/shivam/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting Unidecode\n",
      "  Obtaining dependency information for Unidecode from https://files.pythonhosted.org/packages/e4/63/7685ef40c65aba621ccd2524a24181bf11f0535ab1fdba47e40738eacff6/Unidecode-1.3.7-py3-none-any.whl.metadata\n",
      "  Downloading Unidecode-1.3.7-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
      "\u001B[2K   \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m235.5/235.5 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: Unidecode\n",
      "Successfully installed Unidecode-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T07:02:17.018558024Z",
     "start_time": "2023-09-30T07:02:16.965917516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Segmantation model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 12:32:16.965299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-30 12:32:16.965773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-30 12:32:16.966029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-30 12:32:16.966367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-30 12:32:16.966596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-30 12:32:16.966732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2284 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'import_meta_graph'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m display, clear_output\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Import costume functions, corresponding to notebooks\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mocr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m charSeg\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mocr\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnormalization\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m letterNorm, imageNorm\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# from ocr import charSeg\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Helpers\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Github/Medical-Prescription-OCR/Model-5/ocr/charSeg.py:12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Preloading trained model with activation function\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Loading is slow -> prevent multiple loads\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading Segmantation model:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m segCNNGraph \u001B[38;5;241m=\u001B[39m \u001B[43mGraph\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodels/gap-clas/CNN-CG\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m segRNNGraph \u001B[38;5;241m=\u001B[39m Graph(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/gap-clas/RNN/Bi-RNN-new\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclassify\u001B[39m(img, step\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, RNN\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, slider\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m60\u001B[39m, \u001B[38;5;241m60\u001B[39m)):\n",
      "File \u001B[0;32m~/Documents/Github/Medical-Prescription-OCR/Model-5/ocr/tfhelpers.py:22\u001B[0m, in \u001B[0;36mGraph.__init__\u001B[0;34m(self, loc, operation, input_name)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msess \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mv1\u001B[38;5;241m.\u001B[39mSession(graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mas_default():\n\u001B[0;32m---> 22\u001B[0m     saver \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_meta_graph\u001B[49m(loc \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.meta\u001B[39m\u001B[38;5;124m'\u001B[39m, clear_devices\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     23\u001B[0m     saver\u001B[38;5;241m.\u001B[39mrestore(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msess, loc)\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mget_operation_by_name(operation)\u001B[38;5;241m.\u001B[39moutputs[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow._api.v2.train' has no attribute 'import_meta_graph'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "#from abc import ABC, abstractmethod\n",
    "\n",
    "# Import Widgets\n",
    "from ipywidgets import Button, Text, HBox, VBox\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Import costume functions, corresponding to notebooks\n",
    "from ocr import charSeg\n",
    "from ocr.normalization import letterNorm, imageNorm\n",
    "# from ocr import charSeg\n",
    "# Helpers\n",
    "from ocr.helpers import implt, resize, extendImg\n",
    "from ocr.datahelpers import loadWordsData, idx2char\n",
    "from ocr.tfhelpers import Graph\n",
    "from ocr.viz import printProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T07:00:21.227088714Z",
     "start_time": "2023-09-30T07:00:21.220771641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "LANG = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T07:00:25.777979719Z",
     "start_time": "2023-09-30T07:00:25.757635208Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m charClass_1 \u001B[38;5;241m=\u001B[39m \u001B[43mGraph\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/char-clas/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m LANG \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/CharClassifier\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# charClass_2 = Graph('models/char-clas/' + LANG + '/Bi-RNN/model_2', 'prediction')\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# charClass_3 = Graph('models/char-clas/' + LANG + '/Bi-RNN/model_1', 'prediction')\u001B[39;00m\n\u001B[1;32m      5\u001B[0m wordClass \u001B[38;5;241m=\u001B[39m Graph(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/word-clas/\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m LANG \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/WordClassifier2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction_infer\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Graph' is not defined"
     ]
    }
   ],
   "source": [
    "charClass_1 = Graph('models/char-clas/' + LANG + '/CharClassifier')\n",
    "# charClass_2 = Graph('models/char-clas/' + LANG + '/Bi-RNN/model_2', 'prediction')\n",
    "# charClass_3 = Graph('models/char-clas/' + LANG + '/Bi-RNN/model_1', 'prediction')\n",
    "\n",
    "wordClass = Graph('models/word-clas/' + LANG + '/WordClassifier2', 'prediction_infer')\n",
    "#wordClass2 = Graph('models/word-clas/' + LANG + '/SeqRNN/Classifier3', 'word_prediction') # None\n",
    "wordClass3 = Graph('models/word-clas/' + LANG + '/CTC/Classifier2', 'word_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading words...\n",
      "('-> Number of words:', 267)\n",
      " |****************************************| 100.0% \n",
      "()\n",
      "('Number of chars:', 1356)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Piyush_Jena/ai-saturdays/tf/lib/python2.7/site-packages/unidecode/__init__.py:46: RuntimeWarning: Argument <type 'numpy.string_'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n",
      "  _warn_if_not_unicode(string)\n"
     ]
    }
   ],
   "source": [
    "images, labels = loadWordsData('data/test_words/' + LANG + '_raw', loadGaplines=False)\n",
    "\n",
    "for i in range(len(images)):\n",
    "    printProgressBar(i, len(images))\n",
    "    images[i] = imageNorm(\n",
    "        cv2.cvtColor(images[i], cv2.COLOR_GRAY2RGB),\n",
    "        60,\n",
    "        border=False,\n",
    "        tilt=True,\n",
    "        hystNorm=True)\n",
    "\n",
    "if LANG == 'en':\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = unidecode.unidecode(labels[i])\n",
    "print()        \n",
    "print('Number of chars:', sum(len(l) for l in labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xce in position 1: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-ba806d4a814b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mLANG\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'en'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m             \u001B[0mWORDS\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0munidecode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munidecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\" \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\" \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m             \u001B[0mWORDS\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\" \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\" \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Users/Piyush_Jena/ai-saturdays/tf/lib/python2.7/site-packages/unidecode/__init__.pyc\u001B[0m in \u001B[0;36munidecode_expect_ascii\u001B[0;34m(string)\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[0m_warn_if_not_unicode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstring\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m         \u001B[0mbytestring\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstring\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'ASCII'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mUnicodeEncodeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_unidecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstring\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mUnicodeDecodeError\u001B[0m: 'ascii' codec can't decode byte 0xce in position 1: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# Load Words\n",
    "# -*- coding: UTF-8 -*\n",
    "WORDS = {}\n",
    "with open('data/' + LANG + '_50k.txt') as f:\n",
    "    for line in f:\n",
    "        if LANG == 'en':\n",
    "            WORDS[unidecode.unidecode(line.split(\" \")[0])] = int(line.split(\" \")[1])\n",
    "        else:\n",
    "            WORDS[line.split(\" \")[0]] = int(line.split(\" \")[1])\n",
    "WORDS = Counter(WORDS)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    if word in WORDS:\n",
    "        return word\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    \n",
    "    if LANG == 'cz':\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    else:\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cycler(ABC):\n",
    "    \"\"\" Abstract cycler class \"\"\" \n",
    "    def __init__(self,\n",
    "                 images,\n",
    "                 labels,\n",
    "                 charClass,\n",
    "                 stats=\"NO Stats Provided\",\n",
    "                 slider=(60, 15),\n",
    "                 ctc=False,\n",
    "                 seq2seq=False,\n",
    "                 charRNN=False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.charClass = charClass\n",
    "        self.slider = slider\n",
    "        self.totalChars = sum([len(l) for l in labels])\n",
    "        self.ctc = ctc\n",
    "        self.seq2seq = seq2seq\n",
    "        self.charRNN = charRNN\n",
    "        self.stats = stats\n",
    "        \n",
    "        self.evaluate()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def recogniseWord(self, img):\n",
    "        pass\n",
    "    \n",
    "    def countCorrect(self, pred, label, lower=False):\n",
    "        correct = 0\n",
    "        for i in range(min(len(pred), len(label))):\n",
    "            if ((not lower and pred[i] == label[i])\n",
    "                 or (lower and pred[i] == label.lower()[i])):\n",
    "                correct += 1\n",
    "                \n",
    "        return correct        \n",
    "\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\" Evaluate accuracy of the word classification \"\"\"\n",
    "        print()\n",
    "        print(\"STATS:\", self.stats)\n",
    "        print(self.labels[1], ':', self.recogniseWord(self.images[1]))\n",
    "        start_time = time.time()\n",
    "        correctLetters = 0\n",
    "        correctWords = 0\n",
    "        correctWordsCorrection = 0\n",
    "        correctLettersCorrection = 0\n",
    "        for i in range(len(self.images)):\n",
    "            word = self.recogniseWord(self.images[i])\n",
    "            correctLetters += self.countCorrect(word,\n",
    "                                         self.labels[i])\n",
    "            # Correction works only for lower letters\n",
    "            correctLettersCorrection += self.countCorrect(correction(word.lower()),\n",
    "                                                       self.labels[i],\n",
    "                                                       lower=True)\n",
    "            # Words accuracy\n",
    "            if word == self.labels[i]:\n",
    "                correctWords += 1\n",
    "            if correction(word.lower()) == self.labels[i].lower():\n",
    "                correctWordsCorrection += 1\n",
    "\n",
    "        print(\"Correct/Total: %s / %s\" % (correctLetters, self.totalChars))\n",
    "        print(\"Letter Accuracy: %s %%\" % round(correctLetters/self.totalChars * 100, 4))\n",
    "        print(\"Letter Accuracy with Correction: %s %%\" % round(correctLettersCorrection/self.totalChars * 100, 4))\n",
    "        print(\"Word Accuracy: %s %%\" % round(correctWords/len(self.images) * 100, 4))\n",
    "        print(\"Word Accuracy with Correction: %s %%\" % round(correctWordsCorrection/len(self.images) * 100, 4))\n",
    "        print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCycler(Cycler):\n",
    "    \"\"\" Cycle through the words and recognise them \"\"\" \n",
    "    def recogniseWord(self, img):\n",
    "        slider = self.slider\n",
    "        \n",
    "        if self.ctc:\n",
    "            step = 2    # 10 for (60, 60) slider\n",
    "            img = cv2.copyMakeBorder(\n",
    "                img,\n",
    "                0, 0, self.slider[1]//2, self.slider[1]//2,\n",
    "                cv2.BORDER_CONSTANT,\n",
    "                value=[0, 0, 0])\n",
    "            img = extendImg(\n",
    "                img,\n",
    "                (img.shape[0], max(-(-img.shape[1] // step) * step, self.slider[1] + step)))\n",
    "            length = (img.shape[1]-slider[1]) // step\n",
    "            input_seq = np.zeros((1, length, slider[0] * slider[1]), dtype=np.float32)\n",
    "            input_seq[0][:] = [img[:, loc*step: loc*step + slider[1]].flatten()\n",
    "                             for loc in range(length)]\n",
    "            input_seq = input_seq.swapaxes(0, 1)\n",
    "            \n",
    "            pred = self.charClass.eval_feed({'inputs:0': input_seq,\n",
    "                                             'inputs_length:0': [length],\n",
    "                                             'keep_prob:0': 1})[0]\n",
    "            \n",
    "            word = ''\n",
    "            for i in pred:\n",
    "                if word == 0 and i != 0:\n",
    "                    break\n",
    "                else:\n",
    "                    word += idx2char(i)\n",
    "            \n",
    "        else:       \n",
    "            length = img.shape[1]//slider[1]\n",
    "\n",
    "            input_seq = np.zeros((1, length, slider[0] * slider[1]), dtype=np.float32)\n",
    "            input_seq[0][:] = [img[:, loc * slider[1]: (loc+1) * slider[1]].flatten()\n",
    "                               for loc in range(length)]                                \n",
    "            input_seq = input_seq.swapaxes(0, 1)\n",
    "\n",
    "\n",
    "            if self.seq2seq:\n",
    "                targets = np.zeros((1, 1), dtype=np.int32)  \n",
    "                pred = self.charClass.eval_feed({'encoder_inputs:0': input_seq,\n",
    "                                                 'encoder_inputs_length:0': [length],\n",
    "                                                 'decoder_targets:0': targets,\n",
    "                                                 'keep_prob:0': 1})[0]\n",
    "            else:\n",
    "                targets = np.zeros((1, 1, 4096), dtype=np.int32)  \n",
    "                pred = self.charClass.eval_feed({'encoder_inputs:0': input_seq,\n",
    "                                                 'encoder_inputs_length:0': [length],\n",
    "                                                 'letter_targets:0': targets,\n",
    "                                                 'is_training:0': False,\n",
    "                                                 'keep_prob:0': 1})[0]\n",
    "            word = ''\n",
    "            for i in pred:\n",
    "                if word == 1:\n",
    "                    break\n",
    "                else:\n",
    "                    word += idx2char(i, True)\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCycler(Cycler):\n",
    "    \"\"\" Cycle through the words and recognise them \"\"\" \n",
    "    def recogniseWord(self, img):\n",
    "        img = cv2.copyMakeBorder(img,\n",
    "                                 0, 0, 30, 30,\n",
    "                                 cv2.BORDER_CONSTANT,\n",
    "                                 value=[0, 0, 0])\n",
    "        gaps = charSeg.segmentation(img, RNN=True)\n",
    "        \n",
    "        chars = []\n",
    "        for i in range(len(gaps)-1):\n",
    "            char = img[:, gaps[i]:gaps[i+1]]\n",
    "            # TODO None type error after treshold\n",
    "            char, dim = letterNorm(char, is_thresh=True, dim=True)\n",
    "            # TODO Test different values\n",
    "            if dim[0] > 4 and dim[1] > 4:\n",
    "                chars.append(char.flatten())\n",
    "                \n",
    "        chars = np.array(chars)\n",
    "        word = ''\n",
    "        if len(chars) != 0:\n",
    "            if self.charRNN:\n",
    "                pred = self.charClass.eval_feed({'inputs:0': [chars],\n",
    "                                                 'length:0': [len(chars)],\n",
    "                                                 'keep_prob:0': 1})[0]\n",
    "            else:\n",
    "                pred = self.charClass.run(chars)\n",
    "                \n",
    "            for c in pred:\n",
    "                # word += CHARS[charIdx]\n",
    "                word += idx2char(c)        \n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STATS: Seq2Seq\n",
      "spreads : zpasobe\n",
      "Correct/Total: 626 / 1356\n",
      "Letter Accuracy: 46.1652 %\n",
      "Letter Accuracy with Correction: 45.8702 %\n",
      "Word Accuracy: 21.3483 %\n",
      "Word Accuracy with Correction: 29.588 %\n",
      "--- 28.33 seconds ---\n",
      "\n",
      "STATS: Seq2Seq2CNN\n",
      "spreads : spreadds\n",
      "Correct/Total: 830 / 1356\n",
      "Letter Accuracy: 61.2094 %\n",
      "Letter Accuracy with Correction: 61.2094 %\n",
      "Word Accuracy: 28.4644 %\n",
      "Word Accuracy with Correction: 44.1948 %\n",
      "--- 43.31 seconds ---\n",
      "\n",
      "STATS: CTC\n",
      "spreads : spreads\n",
      "Correct/Total: 853 / 1356\n",
      "Letter Accuracy: 62.9056 %\n",
      "Letter Accuracy with Correction: 67.1091 %\n",
      "Word Accuracy: 41.1985 %\n",
      "Word Accuracy with Correction: 56.5543 %\n",
      "--- 36.54 seconds ---\n",
      "\n",
      "STATS: Bi-RNN and CNN\n",
      "spreads : spreads\n",
      "Correct/Total: 1046 / 1356\n",
      "Letter Accuracy: 77.1386 %\n",
      "Letter Accuracy with Correction: 79.2773 %\n",
      "Word Accuracy: 63.2959 %\n",
      "Word Accuracy with Correction: 72.2846 %\n",
      "--- 65.27 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CharCycler at 0x7f1f01150198>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class cycling through words\n",
    "\n",
    "WordCycler(images,\n",
    "           labels,\n",
    "           wordClass,\n",
    "           stats='Seq2Seq',\n",
    "           slider=(60, 2),\n",
    "           seq2seq=True)\n",
    "\n",
    "WordCycler(images,\n",
    "           labels,\n",
    "           wordClass2,\n",
    "           stats='Seq2Seq2CNN',\n",
    "           slider=(60, 2))\n",
    "\n",
    "WordCycler(images,\n",
    "           labels,\n",
    "           wordClass3,\n",
    "           stats='CTC',\n",
    "           slider=(60, 2),\n",
    "           ctc=True)\n",
    "\n",
    "CharCycler(images,\n",
    "           labels,\n",
    "           charClass_1,\n",
    "           stats='Bi-RNN and CNN',\n",
    "           charRNN=False)\n",
    "\n",
    "# Cycler(images,\n",
    "#        labels,\n",
    "#        charClass_2,\n",
    "#        charRNN=True)\n",
    "\n",
    "# Cycler(images,\n",
    "#        labels,\n",
    "#        charClass_3,\n",
    "#        charRNN=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
